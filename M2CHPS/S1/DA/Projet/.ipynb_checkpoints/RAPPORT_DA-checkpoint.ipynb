{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Université de Paris Saclay\n",
    "# Master 2 : Calcul Haute Performance et Simulation\n",
    "## Members : SMAIL Karim\n",
    "**Object : Homework of the module Data & Apprentissage, M2CHPS 2020.**\n",
    "\n",
    "## Report\n",
    "\n",
    "## Introduction : \n",
    "\n",
    "In this report, we will present our solutions for the different exercises that we have solved.\n",
    "The work of these exercises has been shared between the two members of the group, so that we can discuss the problematic and seek together the solution of each exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 01: RANdom SAmple Consensus (RANSAC)\n",
    "\n",
    "* Implémentation from scratch of the RANSAC algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_samples = 100\n",
    "n_outliers = 30\n",
    "\n",
    "def model(x,theta):\n",
    "\treturn x.dot(theta)\n",
    "\n",
    "\n",
    "\n",
    "def RANSAC_Bis(x, y,taille, n, k, t, d):\n",
    "\titerator = 0\n",
    "\tmeilleur_modele = None\n",
    "\tmeilleur_ensemble = None\n",
    "\tmeilleur_ensemble_points = None\n",
    "\tmeilleur_erreur = 1000\n",
    "\n",
    "\twhile iterator < k:\n",
    "\t\t# Definir l'enssemble de points aléatoires\n",
    "\t\tpoints_aleatoires = random.sample(range(0,taille-1), 20)\n",
    "\t\tx_alea = np.zeros(shape=(n,1))\n",
    "\t\ty_alea = np.zeros(shape=(n,1))\n",
    "\t\t\n",
    "\t\tk = 0\n",
    "\t\tfor i in points_aleatoires:\n",
    "\t\t\tx_alea[k] = x[i]\n",
    "\t\t\ty_alea[k] = y[i]\n",
    "\t\t\tk = k+ 1\n",
    "\t\t\n",
    "        \n",
    "\t\t#Parametres du modèle\n",
    "\n",
    "\t\tX = np.hstack((x_alea,np.ones(x_alea.shape)))\n",
    "\t\treg = LinearRegression().fit(X,y_alea)\n",
    "\t\tmodele_possible = reg.coef_\n",
    "\t\tensemble_points = points_aleatoires\n",
    "\n",
    "\t\t#Construction de l'enssemble de points \n",
    "\n",
    "\t\t#Le reste des points de notre dataset\n",
    "\t\tx_reste  =  np.zeros(shape=(taille-n,1))\n",
    "\t\ty_reste =  np.zeros(shape=(taille-n,1))\n",
    "\t\tpoint_rest = np.zeros(shape=(taille-n,1))\n",
    "\t\tk = 0\n",
    "\t\tfor i in range(taille): \n",
    "\t\t\tif i not in points_aleatoires:\n",
    "\t\t\t\tx_reste[k] = x[i]\n",
    "\t\t\t\ty_reste[k] = y[i]\n",
    "\t\t\t\tpoint_rest[k] = i\n",
    "\t\t\t\tk = k + 1\n",
    "\n",
    "\n",
    "\t\tX_reste = np.hstack((x_reste,np.ones(x_reste.shape)))\n",
    "\t\ty_pred = reg.predict(X_reste)\n",
    "\n",
    "\t\t# Rajouter les points qui repondent au critère à l'enssemble de points \n",
    "\n",
    "\t\tfor  point in range(taille-n):\t\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif (math.sqrt((y_reste[point] - y_pred[point] )**2)) < t:\n",
    "\t\t\t\t\tensemble_points.append(point)\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t# la cardinalité de l'ensemble de poits est superieure à d \n",
    "\t\tif len(ensemble_points) > d:\n",
    "\n",
    "\t\t\tx_points   =  np.zeros(shape=(len(ensemble_points) ,1))\n",
    "\t\t\ty_points =  np.zeros(shape=(len(ensemble_points) ,1))\n",
    "\n",
    "\t\t\tk = 0\n",
    "\t\t\tfor i in ensemble_points :\n",
    "\t\t\t\tx_points[k] = x[i] \n",
    "\t\t\t\ty_points[k] = y[i]\n",
    "\t\t\t\tk  = k + 1 \n",
    "\n",
    "\n",
    "\t\t\tX_points= np.hstack((x_points,np.ones(x_points.shape)))\n",
    "\n",
    "\t\t\tx_test = X_points[:len(X_points)-int(len(X_points)*0.7)]\n",
    "\t\t\ty_test = y_points[:len(X_points)-int(len(y_points)*0.7)]\n",
    "\n",
    "\t\t\tx_train =X_points[len(X_points)-int(len(X_points)*0.7):]\n",
    "\t\t\ty_train = y_points[len(X_points)-int(len(y_points)*0.7):]\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t\n",
    "\t\t\treg = LinearRegression().fit(x_train,y_train)\n",
    "\n",
    "\t\t\ty_predict = reg.predict(x_test)\n",
    "\n",
    "\t\t\tmodele_possible = reg.coef_\n",
    "\t\t\terreur = mean_squared_error(y_test, y_predict)\n",
    "\n",
    "\t\t\t#print(erreur)\n",
    "\t\t\tif erreur < meilleur_erreur:\n",
    "\t\t\t\tmeilleur_modele = modele_possible\n",
    "\t\t\t\tmeilleur_ensemble_points = ensemble_points\n",
    "\t\t\t\tmeilleur_erreur = erreur\n",
    "\n",
    "\t\t \n",
    "\t\titerator = iterator + 1\n",
    "\n",
    "\n",
    "\treturn meilleur_modele, meilleur_ensemble_points, meilleur_erreur\n",
    "\n",
    "#x, y = make_regression(n_samples=n_samples, n_features=1, n_informative=1, noise=10)\n",
    "\n",
    "# utilisation des données de Lab2\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "x = diabetes.data[:, np.newaxis, 2]\n",
    "y = diabetes.target\n",
    "\n",
    "# Add outlier data\n",
    "np.random.seed(0)\n",
    "\n",
    "x = np.append(x,3 + 0.5 * np.random.normal(size=(n_outliers, 1)))\n",
    "\n",
    "y = np.append(y,-3 + 0.5 * np.random.normal(size=n_outliers ))\n",
    "\n",
    "x = np.append(x,-3 + 0.5 * np.random.normal(size=(n_outliers, 1)))\n",
    "\n",
    "y = np.append(y,3 + 0.5 * np.random.normal(size=n_outliers ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = x.reshape(x.shape[0],1)\n",
    "y = y.reshape(y.shape[0],1)\n",
    "\n",
    "\n",
    "taille =2*n_outliers + n_samples\n",
    "inlier_mask = n_samples - n_outliers\n",
    "\t\n",
    "n = 20\n",
    "k = 10\n",
    "t = 2\n",
    "d = 20\n",
    "\n",
    "#Our RANSAC Regression\n",
    "meilleur_modele, meilleur_ensemble_points, meilleur_erreur = RANSAC_Bis(x, y,taille, n, k, t, d)\n",
    "\n",
    "\n",
    "#Linear regression, pour la comparaison avec RANSAC\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "\n",
    "#utilisation Ransac implementation of scikit learn\n",
    "ransac = RANSACRegressor()\n",
    "ransac.fit(x, y)\n",
    "\n",
    "\n",
    "#Tracer les lignes \n",
    "line_X = np.arange(x[:inlier_mask].min(), x[:inlier_mask].max())[:, np.newaxis]\n",
    "line_y = lr.predict(line_X)\n",
    "line_y_ransac = ransac.predict(line_X)\n",
    "\n",
    "\n",
    "lw = 2\n",
    "plt.scatter(x, y, color='yellowgreen', marker='o', label='Inliers')\n",
    "plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')\n",
    "plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw, label='RANSAC regressor')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Response\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercice 02:\n",
    "\n",
    "Assume for example the problem where we have a cloud of data points and we want to estimate their \"center\" in a robust way. How would you design a solution to this problem that adopts the RANSAC principle?\n",
    "\n",
    "\n",
    "- First we calculate the error T which will be half of the greatest distance between two points of the set.\n",
    "\n",
    "- We define D as the number of points that we accept to consider a point as a center - D < card(Data ponit-\n",
    "\n",
    "- We choose a point randomly from the data point.\n",
    "\n",
    "- we calculate the distance between this point and the others. If the distance is less than D, add it to the cloud of the chosen center\n",
    "\n",
    "- If the number of points associated with the chosen center is greater than D, we define the latter as the center (the new center will replace the old center defined in a previous iteration if its cloud is greater)\n",
    "\n",
    "- We redo these steps n times (n is defined) and we return to the last iteration the center found\n",
    "\n",
    "\n",
    "Extend this approach to the clustering of the data in a given number of $k$ clusters by considering an existing clustering method, such as $k$-means, as the  internal model learner).\n",
    "\n",
    "\n",
    "\n",
    "Could you imagine other applications?\n",
    "\n",
    "- Recognition of objects is one of the  RANSAC applications.\n",
    "\n",
    "\n",
    "Could you write an abstract pseudocode that expresses the generalization of RANSAC in arbitrary learning problems? \n",
    "\n",
    "The standard RANSAC algorithm consists thoses steps.\n",
    "\n",
    "1)  randomly selects M (a predetermined number) samples, the for each sample estimates a model hypothesis and finds the support (typically, the number of inliers) for this hypothesis.\n",
    "\n",
    "2) The hypothesis with the largest support is thenchosen as a model and all its inliers are used to refine the model parameters.\n",
    "\n",
    "3) The inlier is defined as a data point whose residual is within some threshold T of the hypothesis. The idea is that those M samples include at least one sample which consists of only true correspondences,\n",
    "thus correct hypothesis can be obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 03:\n",
    "\n",
    "** Why can’t we use this process in Time Series: **\n",
    "\n",
    "In the case of time series, the cross-validation is not trivial. We cannot choose random samples and assign them to either the test set or the train set because it makes no sense to use the values from the future to forecast values in the past. In simple word we want to avoid future-looking when we train our model. There is a temporal dependency between observations, and we must preserve that relation during testing.\n",
    "\n",
    "Cross Validation on Time Series:\n",
    "\n",
    "The method that can be used for cross-validating the time-series model is cross-validation on a rolling basis. Start with a small subset of data for training purpose, forecast for the later data points and then checking the accuracy for the forecasted data points. The same forecasted data points are then included as part of the next training dataset and subsequent data points are forecasted.\n",
    "\n",
    "\n",
    "** Algorithm : **\n",
    "\n",
    "dataset : [1,2,3,...,n]\n",
    "\n",
    "What we need to do is to create (n-1) pairs of training/test sets that follow those two rules:\n",
    "\n",
    "every test set contains unique observations\n",
    "observations from the training set occur before their corresponding test set\n",
    "\n",
    "There is only one way to generate such pairs from my dataset. As a result, I get (n-1) pairs of training/test sets:\n",
    "\n",
    "* Training: [1] =>Test: [2]\n",
    "* Training: [1, 2] =>Test: [3]\n",
    "* Training: [1, 2, 3] =>Test: [4]\n",
    "* ... \n",
    "* Training: [1, 2, 3, …,i] =>Test: [i+1] \n",
    "* ... \n",
    "* Training: [1, 2, 3, …,n-1] =>Test: [n]\n",
    "* Compute the average of the accuracies of the (n-1) test fold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 04: SVM multi-class\n",
    "\n",
    "* Implémentation d'un SVM multi-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-153c5c229cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_vs_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\t\n",
    "def one_vs_all_models(x_train,y_train,nb_classes):\n",
    "\t#file to store classifiers \n",
    "\n",
    "\tmodels = []\n",
    "\t#create our classifiers \n",
    "\n",
    "\tfor i in range(nb_classes):\n",
    "\t\t#make  classe \"i\"  positive and the other classes negative in the trainig set \n",
    "\t\ty_binary = y_train.copy()\n",
    "\t\tfor k in range(len(x_train)):\n",
    "\t\t\tif y_train[k] == i : \n",
    "\t\t\t\ty_binary[k] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\ty_binary[k] = 0\n",
    "\n",
    "\t\t#train the model \n",
    "\t\tclf = svm.SVC(probability=True)\n",
    "\t\tclf.fit(x_train, y_binary)\n",
    "\n",
    "\t\t#put the  classifier in a models array\n",
    "\t\tmodels.append(clf)\n",
    "\n",
    "\treturn models\n",
    "\n",
    "\n",
    "\n",
    "#This function make the classification \n",
    "#The in put is the x \n",
    "#The out put is a the number of the classe which x belongs to \n",
    "\n",
    "\n",
    "def predict(x):\n",
    "\t#use the clissifiers in order to pedict the classes\n",
    "\tvalues = []\n",
    "\n",
    "\tfor i in range(nb_classes):\n",
    "\t\tvalues.append(models[i].predict_proba([x])[0][1])\n",
    "\n",
    "\n",
    "\treturn np.argmax(values)\n",
    "\n",
    "\n",
    "\n",
    "# this is an example using Iris data set \n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "train_set = 0.7\n",
    "nb_classes = 3\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train = x[len(x)-int(len(x)*train_set):]\n",
    "y_train = y[len(y)-int(len(y)*train_set):]\n",
    "\n",
    "\n",
    "x_test = x[:len(x)-int(len(x)*train_set)]\n",
    "y_test = y[:len(y)-int(len(y)*train_set)]\n",
    "\n",
    "\n",
    "models = one_vs_all_models(x_train,y_train,nb_classes)\n",
    "print(predict(x_train[80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a simple demonstration on Iris dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-07e40f276a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from itertools import combinations \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_vs_one_models(x_train,y_train,nb_classes):\n",
    "    #file to store classifiers \n",
    "\n",
    "    #create the combinations \n",
    "    comb = combinations(range(nb_classes), 2) \n",
    "    models = []\n",
    "    #Create the models associeted to the list of combinations \n",
    "    for i in list(comb): \n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        for k in range(len(y)):\n",
    "            if y[k] == i[0] :\n",
    "                x_train.append(x[k]) \n",
    "                y_train.append(i[0])\n",
    "            elif y[k] == i[1]:\n",
    "                x_train.append(x[k]) \n",
    "                y_train.append(i[1])\n",
    "\n",
    "        #train the model \n",
    "        clf = svm.SVC(probability=True)\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        #put the  classifier in a models array\n",
    "        models.append(clf)\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "def  predict(x,nb_classes,models):\n",
    "    k = 0\n",
    "    scores = np.zeros(int(nb_classes*(nb_classes-1)/2))\n",
    "    comb = combinations(range(nb_classes), 2) \n",
    "    for el in list(comb):\n",
    "        if models[k].predict([x]) == el[0]:\n",
    "            scores[el[0]] = scores[el[0]] + 1\n",
    "        else:\n",
    "            scores[el[1]] = scores[el[1]] + 1\n",
    "\n",
    "        k = k + 1\n",
    "\n",
    "\n",
    "    return np.argmax(scores)\n",
    "\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "train_set = 0.7\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "nb_classes = 3\n",
    "x_train = x[len(x)-int(len(x)*train_set):]\n",
    "y_train = y[len(y)-int(len(y)*train_set):]\n",
    "\n",
    "\n",
    "x_test = x[:len(x)-int(len(x)*train_set)]\n",
    "y_test = y[:len(y)-int(len(y)*train_set)]\n",
    "\n",
    "\n",
    "models = one_vs_one_models(x_test,y_train,nb_classes)\n",
    "print(predict(x_test[0],nb_classes,models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 05:\n",
    "\n",
    "** Description : **\n",
    "\n",
    "SVM Ensemble is based on training different SVM models using random parts from a train set. After the training phase, the prediction is made by using the SVM ensemble. Every SVM model gives a classification result, and the globale result of SVM Ensemble is the class that the majority of the SVM models give as a result.\n",
    "\n",
    "** Exemple : **\n",
    "\n",
    "We have three different classes A,B,C  and D_train is a training data set .\n",
    "We made five SVM models as followed :\n",
    "\n",
    "* SVM 1 trained by a random subset D1 from D_train \n",
    "* SVM 2 trained by a random subset D2 from D_train \n",
    "* SVM 3 trained by a random subset D3 from D_train \n",
    "* SVM 4 trained by a random subset D4 from D_train \n",
    "* SVM 5 trained by a random subset D5 from D_train \n",
    "\n",
    "After the training phase we will predict the class of X .\n",
    "\n",
    "* SVM 1 give the result : A\n",
    "* SVM 2 give the result : B\n",
    "* SVM 3 give the result : B\n",
    "* SVM 4 give the result : C\n",
    "* SVM 5 give the result : B\n",
    "\n",
    "Finally X belongs to the class B (the majority of the models associate X to the class B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-12e11ca04491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def SVM_Enssemble(nb_classifier,x_train,y_train,percentage):\n",
    "\tmodels = []\n",
    "\tfor i in range(nb_classifier):\n",
    "\t\t#Random sub set\n",
    "\t\tr = random.sample(range(0,len(x_train)-1), int(len(x_train)*percentage)) \n",
    "\t\tx_sub = []\n",
    "\t\ty_sub = []\n",
    "\t\tfor k in range(len(x_train)):\n",
    "\t\t\tif k in r:\n",
    "\t\t\t\tx_sub.append(x_train[k])\n",
    "\t\t\t\ty_sub.append(y_train[k])\n",
    "\n",
    "\t\t#train the model \n",
    "\t\tclf = svm.SVC(probability=True)\n",
    "\t\tclf.fit(x_sub, y_sub)\n",
    "\n",
    "\t\t#put the  classifier in a models array\n",
    "\t\tmodels.append(clf)\n",
    "\n",
    "\n",
    "\treturn models\n",
    "\n",
    "\n",
    "def prediction(x,models,nb_classes):\n",
    "\n",
    "\tcl = np.zeros(nb_classes)\n",
    "\tfor i in range(len(models)):\n",
    "\t\tcl[models[i].predict([x])] = cl[models[i].predict([x])] + 1\n",
    "\n",
    "\treturn np.argmax(cl)\n",
    "\n",
    "\n",
    "def prdict_test(x_test,models,nb_classes):\n",
    "\n",
    "\ty_test = []\n",
    "\tfor i in range(len(x_test)):\n",
    "\t\ty_test.append(prediction(x_test[i],models,nb_classes))\n",
    "\n",
    "\treturn y_test\n",
    "\n",
    "#Perofrmance test  \n",
    "\n",
    "\n",
    "# rapport entre le nombre de hits sur le totale \n",
    "\n",
    "\n",
    "\n",
    "# Test \n",
    "\n",
    "# import some data to play with\n",
    "digit = datasets.load_breast_cancer()\n",
    "\n",
    "train_set = 0.7\n",
    "nb_classes = 3\n",
    "x = digit.data\n",
    "y = digit.target\n",
    "\n",
    "x_train = x[len(x)-int(len(x)*train_set):]\n",
    "y_train = y[len(y)-int(len(y)*train_set):]\n",
    "\n",
    "\n",
    "x_test = x[:len(x)-int(len(x)*train_set)]\n",
    "y_test = y[:len(y)-int(len(y)*train_set)]\n",
    "\n",
    "#models = SVM_Enssemble(20,x_train,y_train,0.2)\n",
    "#y_pred = prdict_test(x_test,models,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def performance(y_test,y_pred):\n",
    "\tcpt = 0\n",
    "\tfor i in range(len(y_test)):\n",
    "\n",
    "\t\tif y_pred[i] == y_test[i]:\n",
    "\t\t\tcpt = cpt +1\n",
    "\n",
    "\treturn (cpt/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incremental cardinality \n",
    "\n",
    "ox  = []\n",
    "oy = []\n",
    "for i in range(2,20):\n",
    "\tmodels = SVM_Enssemble(i,x_train,y_train,0.3)\n",
    "\tox.append(i)\n",
    "\ty_pred = prdict_test(x_test,models,2)\n",
    "\toy.append(performance(y_test,y_pred))\n",
    "\n",
    "\n",
    "Same number of SVMs (20)\n",
    "ox  = []\n",
    "oy = []\n",
    "p = 0.1\n",
    "while p<0.7:\n",
    "\tmodels = SVM_Enssemble(20,x_train,y_train,p)\n",
    "\tox.append(p)\n",
    "\ty_pred = prdict_test(x_test,models,2)\n",
    "\toy.append(performance(y_test,y_pred))\n",
    "\tp = p + 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(ox, oy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commun point between SVM Ensemble and multi-class SVM is that both of them use a set of weak SVMs which are trained to construt a strong model given a better results \n",
    "\n",
    "the dissrence is the way of training our models SVM Ensemble use diffrent random part pf the train-set\n",
    "but the one_vs_on_model or one_vs_ all_model uses a diffrent type of classes to distingush for the traiing phase)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
